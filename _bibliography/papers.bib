---
---

@inproceedings{redan,
  abbr={NSDI},
  title={{RDMA is Turing complete, we just did not know it yet!}},
  author={Reda, Waleed and Canini, Marco and Kosti{\'c}, Dejan and Peter, Simon},
  booktitle={19th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year={2022},
  abstract={It is becoming increasingly popular for distributed systems to exploit offload to reduce load on the CPU. Remote Direct Memory Access (RDMA) offload, in particular, has become popular. However, RDMA still requires CPU intervention for complex offloads that go beyond simple remote memory access. As such, the offload potential is limited and RDMA-based systems usually have to work around such limitations. We present RedN, a principled, practical approach to implementing complex RDMA offloads, without requiring any hardware modifications. Using self-modifying RDMA chains, we lift the existing RDMA verbs interface to a Turing complete set of programming abstractions. We explore what is possible in terms of offload complexity and performance with a commodity RDMA NIC. We show how to integrate these RDMA chains into applications, such as the Memcached key-value store, allowing us to offload complex tasks such as key lookups. RedN can reduce the latency of key-value get operations by up to 2.6× compared to state-of-the-art KV designs that use one-sided RDMA primitives (e.g., FaRM-KV), as well as traditional RPC-over-RDMA approaches. Moreover, compared to these baselines, RedN provides performance isolation and, in the presence of contention, can reduce latency by up to 35× while providing applications with failure resiliency to OS and process crashes.},
  pdf={redn-nsdi22.pdf},
  code={https://github.com/redn-io/RedN},
  selected={true}
}

@inproceedings{kim2021linefs,
  abbr={SOSP},
  title={LineFS: Efficient SmartNIC Offload of a Distributed File System with Pipeline Parallelism},
  author={Kim, Jongyul and Jang, Insu and Reda, Waleed and Im, Jaeseong and Canini, Marco and Kosti{\'c}, Dejan and Kwon, Youngjin and Peter, Simon and Witchel, Emmett},
  booktitle={Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP)},
  year={2021},
  pages={756--771},
  organization={ACM},
  abstract={In multi-tenant systems, the CPU overhead of distributed file systems (DFSes) is increasingly a burden to application performance. CPU and memory interference cause degraded and unstable application and storage performance, in particular for operation latency. Recent client-local DFSes for persistent memory (PM) accelerate this trend. DFS offload to SmartNICs is a promising solution to these problems, but it is challenging to fit the complex demands of a DFS onto simple SmartNIC processors located across PCIe. We present LineFS, a SmartNIC-offloaded, high-performance DFS with support for client-local PM. To fully leverage the SmartNIC architecture, we decompose DFS operations into execution stages that can be offloaded to a parallel datapath execution pipeline on the SmartNIC. LineFS offloads CPU-intensive DFS tasks, like replication, compression, data publication, index and consistency management to a Smart-NIC. We implement LineFS on the Mellanox BlueField Smart-NIC and compare it to Assise, a state-of-the-art PM DFS. LineFS improves latency in LevelDB up to 80% and throughput in Filebench up to 79%, while providing extended DFS availability during host system failures.},
  pdf={linefs-sosp21.pdf},
  code={https://github.com/casys-kaist/LineFS},
  selected={true}
}

@inproceedings{assise,
  abbr={OSDI},
  title={{Assise: Performance and Availability via Client-local NVM in a Distributed File System}},
  author={Anderson, Thomas E and Canini, Marco and Kim, Jongyul and Kosti{\'c}, Dejan and Kwon, Youngjin and Peter, Simon and Reda, Waleed* and Schuh, Henry N and Witchel, Emmett},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  pages={1011--1027},
  year={2020},
  doi={10.5555/3488766.3488823},
  abstract={The adoption of low latency persistent memory modules (PMMs) upends the long-established model of remote storage for distributed file systems. Instead, by colocating computation with PMM storage, we can provide applications with much higher IO performance, sub-second application failover, and strong consistency. To demonstrate this, we built the Assise distributed file system, based on a persistent, replicated coherence protocol that manages client-local PMM as a linearizable and crash-recoverable cache between applications and slower (and possibly remote) storage. Assise maximizes locality for all file IO by carrying out IO on process-local, socket-local, and client-local PMM whenever possible. Assise minimizes coherence overhead by maintaining consistency at IO operation granularity, rather than at fixed block sizes.
We compare Assise to Ceph/BlueStore, NFS, and Octopus on a cluster with Intel Optane DC PMMs and SSDs for common cloud applications and benchmarks, such as LevelDB, Postfix, and FileBench. We find that Assise improves write latency up to 22x, throughput up to 56x, fail-over time up to 103x, and scales up to 6x better than its counterparts, while providing stronger consistency semantics.},
  pdf = {assise-osdi20.pdf},
  code = {https://github.com/ut-osa/assise},
  addendum={* Lead student author},
  selected={true}
}

@article{rambler,
  abbr={SIGCOMM},
  title={{Path Persistence in the Cloud: A Study of the Effects of Inter-Region Traffic Engineering in a Large Cloud Provider's Network}},
  author={Reda, Waleed and Bogdanov, Kirill and Milolidakis, Alexandros and Ghasemirahni, Hamid and Chiesa, Marco and Maguire Jr, Gerald Q and Kosti{\'c}, Dejan},
  journal={ACM SIGCOMM Computer Communication Review},
  year={2020},
  publisher={ACM New York, NY, USA},
  pages = {11–23},
  doi = {10.1145/3402413.3402416},
  abstract = {A commonly held belief is that traffic engineering and routing changes are infrequent. However, based on our measurements over a number of years of traffic between data centers in one of the largest cloud provider's networks, we found that it is common for flows to change paths at ten-second intervals or even faster. These frequent path and, consequently, latency variations can negatively impact the performance of cloud applications, specifically, latency-sensitive and geo-distributed applications. Our recent measurements and analysis focused on observing path changes and latency variations between different Amazon aws regions. To this end, we devised a path change detector that we validated using both ad hoc experiments and feedback from cloud networking experts. The results provide three main insights: (1) Traffic Engineering (TE) frequently moves (TCP and UDP) flows among network paths of different latency, (2) Flows experience unfair performance, where a subset of flows between two machines can suffer large latency penalties (up to 32% at the 95th percentile) or excessive number of latency changes, and (3) Tenants may have incentives to selfishly move traffic to low latency classes (to boost the performance of their applications). We showcase this third insight with an example using rsync synchronization. To the best of our knowledge, this is the first paper to reveal the high frequency of TE activity within a large cloud provider's network. Based on these observations, we expect our paper to spur discussions and future research on how cloud providers and their tenants can ultimately reconcile their independent and possibly conflicting objectives. Our data is publicly available for reproducibility and further analysis at http://goo.gl/25BKte.},
  pdf = {rambler-ccr20.pdf},
  selected={true}
}

@inproceedings{bogdanov2018fast,
  abbr={SOCC},
  title={Fast and accurate load balancing for geo-distributed storage systems},
  author={Bogdanov, Kirill L and Reda, Waleed and Maguire Jr, Gerald Q and Kosti{\'c}, Dejan and Canini, Marco},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={386--400},
  abstract={The increasing density of globally distributed datacenters reduces the network latency between neighboring datacenters and allows replicated services deployed across neighboring locations to share workload when necessary, without violating strict Service Level Objectives (SLOs). We present Kurma, a practical implementation of a fast and accurate load balancer for geo-distributed storage systems. At run-time, Kurma integrates network latency and service time distributions to accurately estimate the rate of SLO violations for requests redirected across geo-distributed datacenters. Using these estimates, Kurma solves a decentralized rate-based performance model enabling fast load balancing (in the order of seconds) while taming global SLO violations. We integrate Kurma with Cassandra, a popular storage system. Using real-world traces along with a geo-distributed deployment across Amazon EC2, we demonstrate Kurma's ability to effectively share load among datacenters while reducing SLO violations by up to a factor of 3 in high load settings or reducing the cost of running the service by up to 17%.},
  pdf={kurma-socc18.pdf},
  year={2018}
}

@inproceedings{rein,
  abbr={EuroSys},
  title={{Rein: Taming Tail Latency in Key-Value Stores via Multiget Scheduling}},
  author={Reda, Waleed and Canini, Marco and Suresh, Lalith and Kosti{\'c}, Dejan and Braithwaite, Sean},
  booktitle={Twelfth European Conference on Computer Systems (EuroSys)},
  year={2017},
  organization={ACM},
  doi = {10.1145/3064176.3064209},
  pages = {95–110},
  abstract = {We tackle the problem of reducing tail latencies in distributed key-value stores, such as the popular Cassandra database. We focus on workloads of multiget requests, which batch together access to several data elements and parallelize read operations across the data store machines. We first analyze a production trace of a real system and quantify the skew due to multiget sizes, key popularity, and other factors. We then proceed to identify opportunities for reduction of tail latencies by recognizing the composition of aggregate requests and by carefully scheduling bottleneck operations that can otherwise create excessive queues. We design and implement a system called Rein, which reduces latency via inter-multiget scheduling using low overhead techniques. We extensively evaluate Rein via experiments in Amazon Web Services (AWS) and simulations. Our scheduling algorithms reduce the median, 95th, and 99th percentile latencies by factors of 1.5, 1.5, and 1.9, respectively.},
  pdf={rein-eurosys17.pdf},
  selected={true}
}


