<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Waleed  Reda


  | Publications

</title>
<meta name="description" content="Personal webpage">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<!-- Uncommented -->
<!--  -->
<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>"> -->
<!--   -->

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-dark navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://wreda.github.io/">
       <span class="font-weight-bold" style="color:white">Waleed</span>   Reda
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/docs/cv.pdf">
                CV
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">List of conference/journal publications (grouped by year)</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NSDI</abbr>
    
  
  </div>

  <div id="redan" class="col-sm-8">
    
      <div class="title">RDMA is Turing complete, we just did not know it yet!</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Reda, Waleed</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Canini, Marco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kostić, Dejan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Peter, Simon
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</em>
      
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/papers/redn-nsdi22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
      <a href="https://redn.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It is becoming increasingly popular for distributed systems to exploit offload to reduce load on the CPU. Remote Direct Memory Access (RDMA) offload, in particular, has become popular. However, RDMA still requires CPU intervention for complex offloads that go beyond simple remote memory access. As such, the offload potential is limited and RDMA-based systems usually have to work around such limitations. We present RedN, a principled, practical approach to implementing complex RDMA offloads, without requiring any hardware modifications. Using self-modifying RDMA chains, we lift the existing RDMA verbs interface to a Turing complete set of programming abstractions. We explore what is possible in terms of offload complexity and performance with a commodity RDMA NIC. We show how to integrate these RDMA chains into applications, such as the Memcached key-value store, allowing us to offload complex tasks such as key lookups. RedN can reduce the latency of key-value get operations by up to 2.6× compared to state-of-the-art KV designs that use one-sided RDMA primitives (e.g., FaRM-KV), as well as traditional RPC-over-RDMA approaches. Moreover, compared to these baselines, RedN provides performance isolation and, in the presence of contention, can reduce latency by up to 35× while providing applications with failure resiliency to OS and process crashes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SOSP</abbr>
    
  
  </div>

  <div id="kim2021linefs" class="col-sm-8">
    
      <div class="title">LineFS: Efficient SmartNIC Offload of a Distributed File System with Pipeline Parallelism</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Kim, Jongyul,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jang, Insu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Reda, Waleed</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Im, Jaeseong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Canini, Marco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kostić, Dejan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kwon, Youngjin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Peter, Simon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Witchel, Emmett
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP)</em>
      
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/papers/linefs-sosp21.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
      <a href="https://github.com/casys-kaist/LineFS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
      <a class="btn btn-sm z-depth-0" role="button"><img src="/assets/img/trophy.png" border="0" align="abscenter" height="15">  Best Paper</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In multi-tenant systems, the CPU overhead of distributed file systems (DFSes) is increasingly a burden to application performance. CPU and memory interference cause degraded and unstable application and storage performance, in particular for operation latency. Recent client-local DFSes for persistent memory (PM) accelerate this trend. DFS offload to SmartNICs is a promising solution to these problems, but it is challenging to fit the complex demands of a DFS onto simple SmartNIC processors located across PCIe. We present LineFS, a SmartNIC-offloaded, high-performance DFS with support for client-local PM. To fully leverage the SmartNIC architecture, we decompose DFS operations into execution stages that can be offloaded to a parallel datapath execution pipeline on the SmartNIC. LineFS offloads CPU-intensive DFS tasks, like replication, compression, data publication, index and consistency management to a Smart-NIC. We implement LineFS on the Mellanox BlueField Smart-NIC and compare it to Assise, a state-of-the-art PM DFS. LineFS improves latency in LevelDB up to 80% and throughput in Filebench up to 79%, while providing extended DFS availability during host system failures.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">OSDI</abbr>
    
  
  </div>

  <div id="assise" class="col-sm-8">
    
      <div class="title">Assise: Performance and Availability via Client-local NVM in a Distributed File System</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Anderson, Thomas E,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Canini, Marco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kim, Jongyul,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kostić, Dejan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kwon, Youngjin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Peter, Simon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Reda, Waleed*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schuh, Henry N,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Witchel, Emmett
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</em>
      
      
      
        2020
      
      
      </div>
      <div class="periodical">
        <b>* Lead student author</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/papers/assise-osdi20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
      <a href="https://github.com/ut-osa/assise" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The adoption of low latency persistent memory modules (PMMs) upends the long-established model of remote storage for distributed file systems. Instead, by colocating computation with PMM storage, we can provide applications with much higher IO performance, sub-second application failover, and strong consistency. To demonstrate this, we built the Assise distributed file system, based on a persistent, replicated coherence protocol that manages client-local PMM as a linearizable and crash-recoverable cache between applications and slower (and possibly remote) storage. Assise maximizes locality for all file IO by carrying out IO on process-local, socket-local, and client-local PMM whenever possible. Assise minimizes coherence overhead by maintaining consistency at IO operation granularity, rather than at fixed block sizes.
We compare Assise to Ceph/BlueStore, NFS, and Octopus on a cluster with Intel Optane DC PMMs and SSDs for common cloud applications and benchmarks, such as LevelDB, Postfix, and FileBench. We find that Assise improves write latency up to 22x, throughput up to 56x, fail-over time up to 103x, and scales up to 6x better than its counterparts, while providing stronger consistency semantics.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SIGCOMM</abbr>
    
  
  </div>

  <div id="rambler" class="col-sm-8">
    
      <div class="title">Path Persistence in the Cloud: A Study of the Effects of Inter-Region Traffic Engineering in a Large Cloud Provider’s Network</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Reda, Waleed</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bogdanov, Kirill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Milolidakis, Alexandros,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ghasemirahni, Hamid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chiesa, Marco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Maguire Jr, Gerald Q,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kostić, Dejan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ACM SIGCOMM Computer Communication Review</em>
      
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/papers/rambler-ccr20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A commonly held belief is that traffic engineering and routing changes are infrequent. However, based on our measurements over a number of years of traffic between data centers in one of the largest cloud provider’s networks, we found that it is common for flows to change paths at ten-second intervals or even faster. These frequent path and, consequently, latency variations can negatively impact the performance of cloud applications, specifically, latency-sensitive and geo-distributed applications. Our recent measurements and analysis focused on observing path changes and latency variations between different Amazon aws regions. To this end, we devised a path change detector that we validated using both ad hoc experiments and feedback from cloud networking experts. The results provide three main insights: (1) Traffic Engineering (TE) frequently moves (TCP and UDP) flows among network paths of different latency, (2) Flows experience unfair performance, where a subset of flows between two machines can suffer large latency penalties (up to 32% at the 95th percentile) or excessive number of latency changes, and (3) Tenants may have incentives to selfishly move traffic to low latency classes (to boost the performance of their applications). We showcase this third insight with an example using rsync synchronization. To the best of our knowledge, this is the first paper to reveal the high frequency of TE activity within a large cloud provider’s network. Based on these observations, we expect our paper to spur discussions and future research on how cloud providers and their tenants can ultimately reconcile their independent and possibly conflicting objectives. Our data is publicly available for reproducibility and further analysis at http://goo.gl/25BKte.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SOCC</abbr>
    
  
  </div>

  <div id="bogdanov2018fast" class="col-sm-8">
    
      <div class="title">Fast and accurate load balancing for geo-distributed storage systems</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Bogdanov, Kirill L,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Reda, Waleed</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Maguire Jr, Gerald Q,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kostić, Dejan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Canini, Marco
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the ACM Symposium on Cloud Computing</em>
      
      
      
        2018
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/papers/kurma-socc18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The increasing density of globally distributed datacenters reduces the network latency between neighboring datacenters and allows replicated services deployed across neighboring locations to share workload when necessary, without violating strict Service Level Objectives (SLOs). We present Kurma, a practical implementation of a fast and accurate load balancer for geo-distributed storage systems. At run-time, Kurma integrates network latency and service time distributions to accurately estimate the rate of SLO violations for requests redirected across geo-distributed datacenters. Using these estimates, Kurma solves a decentralized rate-based performance model enabling fast load balancing (in the order of seconds) while taming global SLO violations. We integrate Kurma with Cassandra, a popular storage system. Using real-world traces along with a geo-distributed deployment across Amazon EC2, we demonstrate Kurma’s ability to effectively share load among datacenters while reducing SLO violations by up to a factor of 3 in high load settings or reducing the cost of running the service by up to 17%.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EuroSys</abbr>
    
  
  </div>

  <div id="rein" class="col-sm-8">
    
      <div class="title">Rein: Taming Tail Latency in Key-Value Stores via Multiget Scheduling</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Reda, Waleed</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Canini, Marco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Suresh, Lalith,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kostić, Dejan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Braithwaite, Sean
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Twelfth European Conference on Computer Systems (EuroSys)</em>
      
      
      
        2017
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="/papers/rein-eurosys17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We tackle the problem of reducing tail latencies in distributed key-value stores, such as the popular Cassandra database. We focus on workloads of multiget requests, which batch together access to several data elements and parallelize read operations across the data store machines. We first analyze a production trace of a real system and quantify the skew due to multiget sizes, key popularity, and other factors. We then proceed to identify opportunities for reduction of tail latencies by recognizing the composition of aggregate requests and by carefully scheduling bottleneck operations that can otherwise create excessive queues. We design and implement a system called Rein, which reduces latency via inter-multiget scheduling using low overhead techniques. We extensively evaluate Rein via experiments in Amazon Web Services (AWS) and simulations. Our scheduling algorithms reduce the median, 95th, and 99th percentile latencies by factors of 1.5, 1.5, and 1.9, respectively.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    © Copyright 2022 Waleed  Reda.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
